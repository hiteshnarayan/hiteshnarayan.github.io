<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Multimodal Emotion Recognition with Feature Fusion | Hitesh Narayana </title> <meta name="author" content="Hitesh Narayana"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?4bfe0e5d71e88690af3adfb8d882bc27"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_icon.jpeg?abad68b3cecd453e4664cb4b0705e930"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hiteshnarayan.github.io/projects/1_project.html"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Hitesh</span> Narayana </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Multimodal Emotion Recognition with Feature Fusion</h1> <p class="post-description"></p> </header> <article> <h2 id="overview">Overview</h2> <p>This project implements multimodal emotion recognition on the IEMOCAP dataset, classifying utterances into 4 emotion classes using pre-extracted ResNet visual features, VGGish audio features, and BERT text embeddings.The project focused on comparing different feature fusion strategies and understanding when multimodal approaches outperform single-modality methods.</p> <hr> <h2 id="dataset">Dataset</h2> <p>IEMOCAP (Interactive Emotional Dyadic Motion Capture) is a 12‑hour multimodal corpus of dyadic emotional conversations from 10 actors, offering synchronized ResNet‑2048 visual, VGGish‑128 acoustic and BERT‑768 textual features. All features are speaker‑normalized across four emotion classes (Anger, Sadness, Happiness, Neutral) For more details, refer to the <a href="https://sail.usc.edu/iemocap/" rel="external nofollow noopener" target="_blank">IEMOCAP dataset documentation</a>.</p> <p><strong>Core Challenge:</strong> How to effectively combine features from different modalities with different temporal structures.</p> <hr> <h2 id="model-architecture">Model Architecture</h2> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/iemocap/iemocap_model_architecture-480.webp 480w,/assets/img/iemocap/iemocap_model_architecture-800.webp 800w,/assets/img/iemocap/iemocap_model_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/iemocap/iemocap_model_architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Fusion-Based Multimodal Architecture for Emotion Recognition" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <footer> <p class="text-center">Figure: Fusion-Based Multimodal Architecture for Emotion Recognition</p> </footer> <ul> <li>The architecture performs multimodal emotion classification using lexical (BERT), acoustic (VGGish), and visual ( ResNet) features from the IEMOCAP dataset.</li> <li>Temporal mean pooling is applied to align modalities, followed by early or late fusion strategies to combine representations.</li> <li>A neural network classifier predicts one of four emotion classes: Happiness, Anger, Sadness, or Neutral.</li> </ul> <hr> <h2 id="methodology">Methodology</h2> <p>My approach follows a systematic three-stage pipeline: unimodal classification, multimodal fusion, and neural network classification.</p> <h3 id="stage-1-unimodal-feature-processing">Stage 1: Unimodal Feature Processing</h3> <p>Each modality is processed independently to establish baseline performance and understand individual modality contributions:</p> <h4 id="temporal-pooling-for-visual-and-acoustic-features">Temporal Pooling for Visual and Acoustic Features:</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">temporal_pooling</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Reduce temporal dimension using mean pooling
    Args:
        features: Input features with temporal dimension
        method: Pooling method (</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="s"> or </span><span class="sh">'</span><span class="s">max</span><span class="sh">'</span><span class="s">)
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">features</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="n">features</span><span class="p">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="sh">'</span><span class="s">max</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">features</span>

</code></pre></div></div> <p>Next, implement unimodal classifiers for each modality using a simple feedforward neural network architecture.Each classifier is trained independently on its respective modality features.</p> <h4 id="unimodal-neural-network-architecture">Unimodal Neural Network Architecture:</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ModalityClassifier</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ModalityClassifier</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>  <span class="c1"># First hidden layer
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>  <span class="c1"># hidden_dim, hidden_dim // 2),  # Second hidden layer (64 neurons)
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>  <span class="c1"># Output layer (4 emotions)
</span>        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

</code></pre></div></div> <h3 id="stage-2-multimodal-fusion-strategies">Stage 2: Multimodal Fusion Strategies</h3> <p>After establishing unimodal baselines, the next step is to explore different fusion strategies to combine the modalities effectively. This project implements two fusion strategies: early fusion (feature-level) and late fusion (decision-level).</p> <h4 id="early-fusion-feature-level">Early Fusion (Feature-Level):</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EarlyFusionModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">visual_dim</span><span class="p">,</span> <span class="n">acoustic_dim</span><span class="p">,</span> <span class="n">lexical_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">EarlyFusionModel</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># Concatenate all feature dimensions
</span>        <span class="n">self</span><span class="p">.</span><span class="n">combined_dim</span> <span class="o">=</span> <span class="n">visual_dim</span> <span class="o">+</span> <span class="n">acoustic_dim</span> <span class="o">+</span> <span class="n">lexical_dim</span>  <span class="c1"># 2048 + 128 + 768 = 2944
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">combined_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="c1"># Feature concatenation process
</span><span class="k">def</span> <span class="nf">early_fusion_preprocessing</span><span class="p">(</span><span class="n">visual_features</span><span class="p">,</span> <span class="n">acoustic_features</span><span class="p">,</span> <span class="n">lexical_features</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Concatenate normalized features from all modalities
    </span><span class="sh">"""</span>
    <span class="c1"># Normalize each modality separately
</span>    <span class="n">visual_scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
    <span class="n">acoustic_scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
    <span class="n">lexical_scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>

    <span class="n">visual_normalized</span> <span class="o">=</span> <span class="n">visual_scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">visual_features</span><span class="p">)</span>
    <span class="n">acoustic_normalized</span> <span class="o">=</span> <span class="n">acoustic_scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">acoustic_features</span><span class="p">)</span>
    <span class="n">lexical_normalized</span> <span class="o">=</span> <span class="n">lexical_scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">lexical_features</span><span class="p">)</span>

    <span class="c1"># Concatenate along feature dimension
</span>    <span class="n">fused_features</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">visual_normalized</span><span class="p">,</span> <span class="n">acoustic_normalized</span><span class="p">,</span> <span class="n">lexical_normalized</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">fused_features</span>

</code></pre></div></div> <h4 id="late-fusion-decision-level">Late Fusion (Decision-Level):</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">late_fusion_majority_vote</span><span class="p">(</span><span class="n">visual_preds</span><span class="p">,</span> <span class="n">acoustic_preds</span><span class="p">,</span> <span class="n">lexical_preds</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Combine predictions from individual modality classifiers using majority voting
    </span><span class="sh">"""</span>
    <span class="c1"># Stack predictions from all modalities
</span>    <span class="n">stacked_preds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">visual_preds</span><span class="p">,</span> <span class="n">acoustic_preds</span><span class="p">,</span> <span class="n">lexical_preds</span><span class="p">])</span>

    <span class="c1"># Apply majority voting for each sample
</span>    <span class="n">majority_votes</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">apply_along_axis</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">bincount</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="mi">4</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(),</span>  <span class="c1"># 4 emotion classes
</span>        <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">arr</span><span class="o">=</span><span class="n">stacked_preds</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">majority_votes</span>
</code></pre></div></div> <h3 id="stage-3-training-and-evaluation-pipeline">Stage 3: Training and Evaluation Pipeline</h3> <p>The final stage involves training the multimodal model and evaluating its performance using subject-independent cross-validation. This ensures that the model generalizes well across different speakers in the IEMOCAP dataset.</p> <h4 id="cross-validation-setup">Cross-Validation Setup:</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">subject_independent_cv</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">speakers</span><span class="p">,</span> <span class="n">n_folds</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    5-fold subject-independent cross-validation
    Ensures no speaker overlap between train and test sets
    </span><span class="sh">"""</span>
    <span class="n">unique_speakers</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">speakers</span><span class="p">)</span>
    <span class="n">speaker_groups</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array_split</span><span class="p">(</span><span class="n">unique_speakers</span><span class="p">,</span> <span class="n">n_folds</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">fold</span><span class="p">,</span> <span class="n">test_speakers</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">speaker_groups</span><span class="p">):</span>
        <span class="n">test_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">isin</span><span class="p">(</span><span class="n">speakers</span><span class="p">,</span> <span class="n">test_speakers</span><span class="p">)</span>
        <span class="n">train_mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">test_mask</span>

        <span class="k">yield</span> <span class="n">train_mask</span><span class="p">,</span> <span class="n">test_mask</span><span class="p">,</span> <span class="n">fold</span>
</code></pre></div></div> <p><em>Note:</em></p> <h4 id="training-configuration">Training Configuration:</h4> <ul> <li>Optimizer: Adam (learning_rate=0.001, weight_decay=1e-5)</li> <li>Loss Function: CrossEntropyLoss</li> <li>Batch Size: 32</li> <li>Epochs: 50 with early stopping</li> <li>Evaluation Metric: F1-micro score for balanced class assessment</li> </ul> <h2 id="results">Results</h2> <p>Evaluated using 5-fold subject-independent cross-validation:</p> <hr> <table> <thead> <tr> <th>Method</th> <th>F1-Score</th> </tr> </thead> <tbody> <tr> <td>Early Fusion</td> <td>62.56%</td> </tr> <tr> <td>Text Only</td> <td>62.25%</td> </tr> <tr> <td>Late Fusion</td> <td>54.31%</td> </tr> <tr> <td>Audio Only</td> <td>53.27%</td> </tr> <tr> <td>Visual Only</td> <td>38.39%</td> </tr> </tbody> </table> <hr> <p><strong>Key Findings:</strong></p> <ul> <li>The early fusion method outperforms the unimodal features and late fusion method, with a mean F1 score of 62.56%.</li> <li>Early has the higher F-1, which is likely more effective because it integrates the different modalities’ features before any processing, allowing the model to capture relationships between them more effectively.</li> <li>Apparently, combining features at an earlier stage may provide a more holistic representation of the data, improved classification performance.</li> </ul> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/iemocap/performance_comparison-480.webp 480w,/assets/img/iemocap/performance_comparison-800.webp 800w,/assets/img/iemocap/performance_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/iemocap/performance_comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Performance Comparison Across Methods" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The above figure illustrates the performance comparison across different methods, highlighting the superiority of early fusion.</p> <h2><br></h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/iemocap/modality_contribution-480.webp 480w,/assets/img/iemocap/modality_contribution-800.webp 800w,/assets/img/iemocap/modality_contribution-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/iemocap/modality_contribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Individual Modality Contributions" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The above figure shows the individual contributions of each modality to the overall classification performance, indicating that visual features contribute the least, while text and audio features are more informative.</p> <h2 id="references">REFERENCES</h2> <ul> <li><a href="https://sail.usc.edu/iemocap/" rel="external nofollow noopener" target="_blank">IEMOCAP Dataset</a></li> <li>GRATITUDE TO <a href="https://viterbi.usc.edu/directory/faculty/Soleymani/Mohammad/" rel="external nofollow noopener" target="_blank">Prof. Mohammad Soleymani</a> for providing the dataset and guidance on the project.</li> <li> <a href="https://docs.pytorch.org/docs/stable/index.html" rel="external nofollow noopener" target="_blank">PyTorch Documentation</a> for model building and training utilities.</li> <li>Hugging Face’s <a href="https://huggingface.co/transformers/" rel="external nofollow noopener" target="_blank">Transformers</a> library for BERT embeddings.</li> </ul> <h2 id="future-directions">Future Directions</h2> <ol> <li> <strong>Advanced Fusion:</strong> Attention-based mechanisms for dynamic modality weighting</li> <li> <strong>Temporal Modeling:</strong> RNN/Transformer architectures to preserve sequence information</li> <li> <strong>Real-time Deployment:</strong> Optimize for live emotion recognition applications</li> <li> <strong>Cross-dataset Evaluation:</strong> Test generalization on other multimodal emotion datasets</li> </ol> <hr> <h2 id="conclusion">Conclusion</h2> <p><em>This project demonstrates the complexity and potential of multimodal emotion recognition, highlighting both the promise of fusion techniques and the challenges in aligning diverse data modalities for optimal performance.</em></p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Hitesh Narayana. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>